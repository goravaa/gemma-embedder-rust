mod inference;
mod services;

// This module is generated by `build.rs`
pub mod generated;

use crate::generated::inference::inferencer_server::InferencerServer;
use crate::inference::inference_batch_processor;
use crate::services::{AppState, grpc_service::MyInferenceService, http_get_embedding};
use axum::{Router, routing::post};
use hyper::server::conn::http1;
use hyper::service::service_fn;
use hyper_util::rt::TokioIo;
use ort::execution_providers::CUDAExecutionProvider;
use ort::session::{Session, builder::GraphOptimizationLevel};
use std::error::Error;
use std::env;
use std::sync::{Arc, Mutex};
use tokenizers::{PaddingParams, PaddingStrategy, Tokenizer, TruncationParams, TruncationStrategy};
use tokio::net::TcpListener;
use tonic::transport::Server as GrpcServer;
use tonic_reflection::server::Builder;
use tower::ServiceExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error + Send + Sync>> {
    let model_path = std::env::var("MODEL_PATH").unwrap_or_else(|_| {
        eprintln!("MODEL_PATH not set");
        std::process::exit(1);
    });
    println!("Using model: {}", model_path);

    let tokenizer_path = "model/tokenizer.json";
    println!("using tokenizer: {}", tokenizer_path);
    let max_length: usize = env::var("MAX_TOKENS")
        .unwrap_or_else(|_| "2048".to_string()) // default if not set
        .parse()
        .unwrap_or_else(|_| {
            eprintln!("MAX_TOKENS must be a number");
            std::process::exit(1);
        });

    let max_batch_size: usize = env::var("MAX_BATCH_SIZE")
    .unwrap_or_else(|_| "32".to_string())
    .parse()
    .unwrap_or_else(|_| {
        eprintln!("MAX_BATCH_SIZE must be a number");
        std::process::exit(1);
    });

let max_wait_ms: u64 = env::var("MAX_WAIT_MS")
    .unwrap_or_else(|_| "5".to_string())
    .parse()
    .unwrap_or_else(|_| {
        eprintln!("MAX_WAIT_MS must be a number");
        std::process::exit(1);
    });
    let http_addr = "0.0.0.0:3000";
    let grpc_addr = "0.0.0.0:50051";

    println!("Loading models...");

    let mut tokenizer = Tokenizer::from_file(tokenizer_path)?;
    let padding_params = PaddingParams {
        strategy: PaddingStrategy::Fixed(max_length),
        ..Default::default()
    };
    let truncation_params = TruncationParams {
        max_length,
        strategy: TruncationStrategy::LongestFirst,
        ..Default::default()
    };
    tokenizer.with_padding(Some(padding_params));
    tokenizer.with_truncation(Some(truncation_params));

    let session = Session::builder()?
        .with_optimization_level(GraphOptimizationLevel::Level3)?
        .with_execution_providers([CUDAExecutionProvider::default().build()])?
        .commit_from_file(model_path)?;
    
    println!("Running with CUDA execution provider.");
    let shared_tokenizer = Arc::new(tokenizer);
    let shared_session = Arc::new(Mutex::new(session));
    println!("Models loaded successfully.");

    let (inference_sender, inference_receiver) = tokio::sync::mpsc::channel(256);

    tokio::spawn(inference_batch_processor(
        inference_receiver,
        shared_tokenizer.clone(),
        shared_session.clone(),
        max_batch_size,
        max_wait_ms
    ));

    let app_state = Arc::new(AppState { inference_sender });

    let http_app = Router::new()
        .route("/v1/embed", post(http_get_embedding))
        .with_state(app_state.clone());

    let http_future = async move {
        let listener = TcpListener::bind(http_addr).await?;
        println!("HTTP server listening on {}", http_addr);

        loop {
            let (stream, _) = listener.accept().await?;
            let io = TokioIo::new(stream);
            let app_clone = http_app.clone();

            tokio::spawn(async move {
                let service = service_fn(move |req| app_clone.clone().oneshot(req));
                if let Err(err) = http1::Builder::new().serve_connection(io, service).await {
                    eprintln!("HTTP connection error: {:?}", err);
                }
            });
        }

        #[allow(unreachable_code)]
        Ok::<_, Box<dyn Error + Send + Sync>>(())
    };

    let grpc_svc = InferencerServer::new(MyInferenceService { app_state });
    let reflection_svc = Builder::configure()
        .register_encoded_file_descriptor_set(generated::FILE_DESCRIPTOR_SET)
        .build_v1()?;

    let grpc_future = async move {
        GrpcServer::builder()
            .add_service(grpc_svc)
            .add_service(reflection_svc)
            .serve(grpc_addr.parse()?)
            .await
            .map_err(|e| -> Box<dyn Error + Send + Sync> { Box::new(e) })
    };

    println!("gRPC server listening on {}", grpc_addr);

    tokio::try_join!(http_future, grpc_future)?;

    Ok(())
}
