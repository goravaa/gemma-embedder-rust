mod inference;
mod services;

// This module is generated by `build.rs`
pub mod generated;

use crate::generated::inference::inferencer_server::InferencerServer;
use crate::inference::inference_batch_processor;
use crate::services::{AppState, grpc_service::MyInferenceService, http_get_embedding};
use axum::{Router, routing::post};
use ort::session::{Session, builder::GraphOptimizationLevel};
use std::error::Error;
use std::sync::{Arc, Mutex};
use tokenizers::{PaddingParams, PaddingStrategy, Tokenizer, TruncationParams, TruncationStrategy};
use tonic::transport::Server as GrpcServer;
use tonic_reflection::server::Builder;
use hyper::server::conn::http1;
use hyper::service::service_fn;
use tokio::net::TcpListener;
use tower::ServiceExt;
use hyper_util::rt::TokioIo;



#[tokio::main]
async fn main() -> Result<(), Box<dyn Error + Send + Sync>> {
    // ---- 1. Configuration ----
    let model_path = "model/model_quantized.onnx";
    let tokenizer_path = "model/tokenizer.json";
    let max_length = 2048;
    let http_addr = "0.0.0.0:3000";
    let grpc_addr = "0.0.0.0:50051";

    // ---- 2. Load Models and Tokenizer ONCE ----
    println!("Loading models...");

    let mut tokenizer = Tokenizer::from_file(tokenizer_path)?;
    let padding_params = PaddingParams {
        strategy: PaddingStrategy::Fixed(max_length),
        ..Default::default()
    };
    let truncation_params = TruncationParams {
        max_length,
        strategy: TruncationStrategy::LongestFirst,
        ..Default::default()
    };
    tokenizer.with_padding(Some(padding_params));
    tokenizer.with_truncation(Some(truncation_params));

    let session = Session::builder()?
        .with_optimization_level(GraphOptimizationLevel::Level3)?
        .with_intra_threads(num_cpus::get())?
        .commit_from_file(model_path)?;

    let shared_tokenizer = Arc::new(tokenizer);
    let shared_session = Arc::new(Mutex::new(session));
    println!("Models loaded successfully.");

    // ---- 3. Setup Batching Channel and Background Task ----
    let (inference_sender, inference_receiver) = tokio::sync::mpsc::channel(256);

    tokio::spawn(inference_batch_processor(
        inference_receiver,
        shared_tokenizer.clone(),
        shared_session.clone(),
        32,
        5,
    ));

    // ---- 4. Setup Shared State ----
    let app_state = Arc::new(AppState { inference_sender });

    // ---- 5. HTTP Server (Axum over Hyper HTTP/1.1) ----
    let http_app = Router::new()
        .route("/v1/embed", post(http_get_embedding))
        .with_state(app_state.clone());

    let http_future = async move {
        let listener = TcpListener::bind(http_addr).await?;
        println!("HTTP server listening on {}", http_addr);

        loop {
            let (stream, _) = listener.accept().await?;
            let io = TokioIo::new(stream);
            let app_clone = http_app.clone();

            tokio::spawn(async move {
                let service = service_fn(move |req| app_clone.clone().oneshot(req));
                if let Err(err) = http1::Builder::new()
                    .serve_connection(io, service)
                    .await
                {
                    eprintln!("HTTP connection error: {:?}", err);
                }
            });
        }

        #[allow(unreachable_code)]
        Ok::<_, Box<dyn Error + Send + Sync>>(())
    };

    // ---- 6. gRPC Server (Tonic over HTTP/2) ----
    let grpc_svc = InferencerServer::new(MyInferenceService { app_state });
    let reflection_svc = Builder::configure()
        .register_encoded_file_descriptor_set(generated::FILE_DESCRIPTOR_SET)
        .build_v1()?;

    let grpc_future = async move {
        GrpcServer::builder()
            .add_service(grpc_svc)
            .add_service(reflection_svc)
            .serve(grpc_addr.parse()?)
            .await
            .map_err(|e| -> Box<dyn Error + Send + Sync> { Box::new(e) })
    };

    println!("gRPC server listening on {}", grpc_addr);

    // ---- 7. Run Both Servers Concurrently ----
    tokio::try_join!(http_future, grpc_future)?;

    Ok(())
}
